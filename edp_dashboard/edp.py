# -*- coding: utf-8 -*-
"""EDP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D-_ATedetC3YlKnn_b8tIDuQplU_P7Pi
"""

from google.colab import drive

drive.mount('/content/gdrive', force_remount=True)

import pandas as pd
import numpy as np
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from plotly.subplots import make_subplots

#Electric cars chargings Points optimal location
#Domain Energy & Environment

#Identify the optimal points where to install the electric vehicule cchargings points
#EDP is a company that leads the energy sector. in spain, EDP is present in generation, distribution and commercialization of electricity , gas and services.

#the goal is to identify the optimal points where to install the electric vehicule chargings. EDP is now expandings its public charging infrastructure and it is of high importance where to place the nex chargings points.
#the decision will be based on the location of existing points, the trafic information and matriculation of existing cars plus projection of electric cars in the future.

#need spanish traffic information
#EDP own public infrastructure data

#the objective of this challenge is to develop a predictive Model which based on the inputs included in the datasets generates the charging points optimal location. the algorithm should be able of deciding where EDP should install its charging points.
#the best is to have a dashbord that shows in the best possible way the result of the predictive model.

"""#File loading"""



# code qui permet en cas de changement de chemin du repertoire root, pour eviter de retaper tout le chemin
#fonction utilisée: concatenantion
data_dir = './gdrive/My Drive/edp_data/'
mat_file = data_dir + 'matricules.xlsx'
points_file = data_dir + 'PuntosRecarga.xlsx'
provincia_file = data_dir + 'provincia_listado.xlsx'
impagos_file = data_dir + 'impagos.xlsx'

#matricules.xlxs
mat_cols = ['Categoria_veh_elect', 'Año', 'Mercado', 'Marca', 'Modelo', 'Provincia', 'Canal']
df_matricules = pd.read_excel(mat_file,
                              usecols=mat_cols)

""""Canal" : Particular/Empressa/Alquilador"""

#provincia_listado.xlsx
prov_cols = ['Autonomia', 'Superficie', 'PROVINCIA']
df_provincia = pd.read_excel(provincia_file,
                             usecols=prov_cols)

#impagos.xlsx
impagos_cols = ['CONSUMO_ELECTRICO_ANUAL', 'PROVINCIA', 'TIPO_CLIENTE']
df_impagos = pd.read_excel(impagos_file,
                           usecols=impagos_cols)

#PuntosRecarga.xlsx
df_points = pd.read_excel(points_file)

df_impagos.info()

df_matricules.info()

df_provincia.info()

df_points.info()

"""# Data Preprocessing"""

def match_region_name(reg_src, list_reg):
  """
  Matches composite names which appears in a different order
  e.g. 'Principade de Asturias' and 'Asturias Principado de'
  """
  l_src = reg_src.split()

  if len(l_src) == 1:
    return reg_src

  l = list(map(str.split, list_reg))
  l = [i for i in l if len(i) > 1]
  return ' '.join(next((s for s in l if set(l_src) == set(s)), reg_src.split()))

def replace_whole_str(sub, l):
  return next((s for s in l if sub in s), sub)

#Normalisation des regions du fichier points
df_points['region'] = df_points['region'].str.lower()
df_points['region'] = df_points['region'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')
df_points['region'].unique()

#Normalisation des noms de provinces du fichier provincia_listado
df_provincia['PROVINCIA'] = df_provincia['PROVINCIA'].str.lower()
df_provincia['PROVINCIA'] = df_provincia['PROVINCIA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')
df_provincia['PROVINCIA'] = df_provincia['PROVINCIA'].str.replace('\(|\)', '', regex=True)

df_provincia['Autonomia'] = df_provincia['Autonomia'].str.lower()
df_provincia['Autonomia'] = df_provincia['Autonomia'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')

df_provincia['Autonomia'] = df_provincia['Autonomia'].str.replace('\(|\)', '', regex=True)
df_provincia['Autonomia'] = df_provincia['Autonomia'].apply(lambda x : match_region_name(x, df_points['region'].unique()))
df_provincia['Autonomia'].unique()

#Impagos normalisation
df_impagos['PROVINCIA'] = df_impagos['PROVINCIA'].str.lower()
df_impagos['PROVINCIA'] = df_impagos['PROVINCIA'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')
#df_impagos['PROVINCIA'] = df_impagos['PROVINCIA'].apply(lambda x : x.replace(' / ', '/'))
df_impagos['PROVINCIA'] = df_impagos['PROVINCIA'].str.replace(' / ', '/')
df_impagos['PROVINCIA'] = df_impagos['PROVINCIA'].apply(lambda x : replace_whole_str(x, df_provincia['PROVINCIA'].unique()))
df_impagos['PROVINCIA'] = df_impagos['PROVINCIA'].apply(lambda x : match_region_name(x, df_provincia['PROVINCIA'].unique()))
df_impagos['PROVINCIA'].unique()

df_provincia['PROVINCIA'].unique()

#Normalisation des noms des provinces du fichier matricules
df_matricules['Provincia'] = df_matricules['Provincia'].apply(lambda x : x.strip().lower())
df_matricules['Provincia'] = df_matricules['Provincia'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')
df_matricules['Provincia'] = df_matricules['Provincia'].str.replace('\(|\)', '', regex=True)
df_matricules['Provincia'] = df_matricules['Provincia'].apply(lambda x : replace_whole_str(x, df_provincia['PROVINCIA'].unique()))
df_matricules['Provincia'].unique()

#Sanity check
try:
  assert set(df_matricules['Provincia'].unique()).issubset(set(df_provincia['PROVINCIA'].unique()))
  print('Provincia names from matricules.xlsx MATCH names from provincia_listado.xlsx')
except AssertionError:
  print('Provincia names from matricules.xlsx DO NOT MATCH names from provincia_listado.xlsx')

print('#'*50)

try:
  assert(set(df_points['region'].unique()).issubset(set(df_provincia['Autonomia'].unique())))
  print('Autonomia names from PuntosRecarga MATCH names from provincia_listado.xlsx')
except AssertionError:
  print('Autonomia names from PuntosRecarga.xlsx DO NOT MATCH names from provincia_listado.xlsx')

print('#'*50)

try:
  assert(set(df_impagos['PROVINCIA'].unique()).issubset(set(df_provincia['PROVINCIA'].unique())))
  print('Provincia names from impagos.xlsx MATCH names from  provincia_listado.xlsx')
except AssertionError:
  print('Provincia names from impagos.xlsx DO NOT MATCH names from  provincia_listado.xlsx')

# cette celule est à enlever éventuellement
# Get not matching elements
print(set(df_matricules['Provincia'].unique()).difference(set(df_provincia['PROVINCIA'].unique())))

#Join matricules w/ provincia to add Autonomia
df_merged_w_provincia= df_matricules.merge(df_provincia,
                                left_on='Provincia',
                                right_on='PROVINCIA')

df_merged_w_provincia.head()

df_merged_w_provincia.info()

#Join points w/ matricules
df_merged_w_points = df_merged_w_provincia.merge(df_points,
                                                 how='left',
                                                 left_on='Autonomia',
                                                 right_on='region')

#Df n'ayant pas de bornes dans la même regions que les voitures
#df_merged_w_points[df_merged_w_points['zona'].isnull()].info() # 56 entries
df_merged_w_points[df_merged_w_points['zona'].isnull()].head()

#Df bornes présentes dans la même région que les voitures
#df_merged_w_points[df_merged_w_points['zona'].notnull()].info() # 868 entries
df_merged_w_points[df_merged_w_points['zona'].notnull()].head()

"""#Exploratory Data Analysis

##Data matricules/points
"""

#Nombre de bornes par region
df_point_per_region = df_points['region'].value_counts().rename_axis('region').reset_index(name='count_point')
df_point_per_region

#Nb matricule par region
df_veh_per_region = df_merged_w_provincia.groupby('Autonomia').agg({'Categoria_veh_elect': 'count'}).reset_index()
df_veh_per_region

#Par region, nombre de bornes et voiture (Left Join)
df_merged = df_provincia.merge(df_point_per_region, how='left', left_on='Autonomia', right_on='region')\
                        .merge(df_veh_per_region, how='left', on='Autonomia')
df_merged = df_merged[['Autonomia', 'count_point', 'Categoria_veh_elect']].drop_duplicates().fillna(0)
df_merged.rename(columns={'Categoria_veh_elect': 'count_veh'}, inplace=True)
df_merged

df_merged.info()

x = df_merged['Autonomia'].values.tolist()
fig = make_subplots(rows=2, cols=1, 
                    subplot_titles=['Number of vehicle per autonomia', 'Number of points per autonomia'])

#Number of vehicle per autonomia
fig.add_trace(go.Bar(x=x, y=df_merged['count_veh'].values.tolist(), text=df_merged['count_veh'].values.tolist(), textposition='outside', name='nb_vehicle'),
              row=1,
              col=1)


#Number of points per autonomia
fig.add_trace(go.Bar(x=x, y=df_merged['count_point'].values.tolist(), text=df_merged['count_point'].values.tolist(), textposition='outside', name='nb_poins'),
              row=2,
              col=1)

fig.update_layout(height=900, width=1300)
fig.show()

df_veh_per_marca = df_matricules.groupby('Marca').agg({'Categoria_veh_elect': 'count'}).reset_index()
df_veh_per_marca

"""The number of vehicle is low and contains only Volkswagen cars for some reason. </br>
This file must be incomplete.
"""

autonomias = df_merged['Autonomia'].unique()

fig = go.Figure(data=[
    go.Bar(name='nb_points', x=autonomias, y=df_merged['count_point']),
    go.Bar(name='nb_vehicles', x=autonomias, y=df_merged['count_veh'])
])
# Change the bar mode
fig.update_layout(title='Number of points and vehicle per autonomia', barmode='group')
fig.show()

"""We can see that some cities have electrical vehicles but don't have charging points. <br/>
Notice that Asturias has too many charging points (120+ points for 10- vehicles)

## Data Impagos
"""

s_counts = df_impagos['PROVINCIA'].value_counts()
data = {'PROVINCIA': s_counts.index.tolist(), 
        'count_veh': s_counts.values.tolist()}
df_counts = pd.DataFrame(data, columns=['PROVINCIA', 'count_veh'])

y = df_counts['count_veh'].values.tolist()

fig = go.Figure(go.Bar(x=df_counts['PROVINCIA'].values.tolist(),
                       y=y,
                       text=y,
                       textposition='outside'))

fig.update_layout(title='Number of vehicle per autonomia according impagos file')
fig.show()

"""This barplot show electric/hybrid car users. Most of the customers are from Asturias, which is expected. <br/>
As shown above, Asturias has most of the charging points but in this file, we have more users coming from there. <br/>
"""

df_impagos = df_impagos[df_impagos['TIPO_CLIENTE'].isin(['ELEC', 'DUAL'])]
df_impagos_consumo = df_impagos.groupby('PROVINCIA').agg({'CONSUMO_ELECTRICO_ANUAL': 'sum'}).reset_index()
df_impagos = df_impagos.merge(df_provincia, on='PROVINCIA')
df_impagos = df_impagos.groupby('Autonomia').agg({'CONSUMO_ELECTRICO_ANUAL': 'sum'}).reset_index()
df_impagos.sort_values('CONSUMO_ELECTRICO_ANUAL', inplace=True)

#Plot electrical consumption per autonomia
y = df_impagos_consumo['CONSUMO_ELECTRICO_ANUAL'].values.tolist()

fig = go.Figure(go.Bar(x=df_impagos_consumo['Autonomia'].values.tolist(),
                       y=y,
                       text=y,
                       textposition='outside',
                       texttemplate='%{y:2.2f}'))

fig.update_layout(title='Electrical consumption per autonomia')
fig.show()

#labels = df_provincia.Autonomia.drop_duplicates().tolist()
labels = df_merged.Autonomia.drop_duplicates().tolist()
x = [0] * len(labels) 
for index_i, row_i in df_merged.iterrows():
    if row_i["Autonomia"] in labels:
        x[labels.index(row_i["Autonomia"])] == row_i["count_point"]
    else: 
        print("Autonomia not found : {}".format(row_i))

import plotly.express as px

fig = px.pie(df_merged[["Autonomia", "count_veh"]], values='count_veh', names='Autonomia', title='Distribution of cars', hole=.1)
fig.update_traces(textposition='inside', textinfo='value+percent+label')

fig.show()

import plotly.express as px

fig = px.pie(df_merged[["Autonomia", "count_point"]], values='count_point', names='Autonomia', title='Distribution of Points', hole=.1)
fig.update_traces(textposition='inside', textinfo='value+percent+label')

fig.show()

print(labels,x)    
# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=1)

fig = make_subplots(rows=1, cols=1, subplot_titles=['Pie Chart'])
fig.add_trace(go.Pie(values=x, labels=labels, name="Points Number"),1, 1)
#fig.add_trace(go.Bar(x=x, y=df_merged['count_point'].values.tolist(), text=df_merged['count_point'].values.tolist(), textposition='outside', name='nb_poins'), row=2,col=1)


# Use `hole` to create a donut-like pie chart
#fig.update_traces(hole=.5, hoverinfo="label+percent+name")

fig.update_layout(
    title_text="Pie Chart",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Pie Chart', x=0.18, y=0.5, font_size=20, showarrow=False)])

#fig.update_layout(height=900, width=1300)
fig.show()

# pour les différents type de bornes

"""## Exploring distances between points

1 - Looking the distribution of distance between points
"""

# Plot distances between pooint to see if it's something relevant 
# for each point compute distance with the closet other point
df_points_dist = df_merged_w_points[["Autonomia","zona","latitud", "longitud"]]
df_points_dist = df_points_dist.dropna().drop_duplicates()
df_points_dist.info()

import geopy.distance
distance_list = []
for index_i, row_i in df_points_dist.iterrows() :
    list_dist = [geopy.distance.vincenty((row_i["latitud"], row_i["longitud"]), (row_j["latitud"], row_j["longitud"])).km 
                 for index_j, row_j in df_points_dist.iterrows() if index_i != index_j 
                 ]
    distance_list.append(min(list_dist)) 
df_points_dist["distances"] = distance_list

fig, axs = plt.subplots(1,2, figsize=(25,8))
axs[0].hist(x = df_points_dist.distances, bins=30, rwidth=0.9, color='#636efa')
#df_points_dist.distances.plot.hist(grid=True, bins=10, rwidth=0.9, color='#636efa')
axs[0].set_title('Distances between the nearest points')
axs[0].set_xlabel('distance(km)')
axs[0].set_ylabel('Point number')
#axs[0].set_grid(axis='y', alpha=0.75)

axs[1].boxplot(df_points_dist.distances)
axs[1].set_title('Boxplot for nearest distances between points')

print("mean {} km".format(np.mean(df_points_dist.distances)))
print("variance {}".format(np.var(df_points_dist.distances)))

import scipy.stats as scp
df_points_dist_per_region = df_points_dist['Autonomia'].value_counts().rename_axis('Autonomia').reset_index(name='count_point')
#df_points_dist_per_region["mean"] = [df_points_dist[df_points_dist["Autonomia"] == row_i["Autonomia"]].distances.mean for index_i, row_i in df_points_dist_per_region.iterrows()]
df_points_dist_per_region["mean"] = [np.mean(df_points_dist[df_points_dist["Autonomia"] == row_i["Autonomia"]].distances.tolist()) 
                                      for index_i, row_i in df_points_dist_per_region.iterrows()
                                    ]
df_points_dist_per_region["sd_deviation"] = [scp.sem(df_points_dist[df_points_dist["Autonomia"] == row_i["Autonomia"]].distances.tolist()) 
                                      for index_i, row_i in df_points_dist_per_region.iterrows()
                                    ]

#h = se * scp.t.ppf((1 + confidence) / 2., n-1)
    #return m, m-h, m+h                         
df_points_dist_per_region

nb_autonomia=0
for index_i, row_i in df_points_dist_per_region.iterrows():
  if row_i.count_point > 2:
    nb_autonomia+=1
nb_autonomia
#plt.subplots(nb_autonomia, sharey = True)
ncols = 2
nrows = nb_autonomia//2 + nb_autonomia%2
fig = plt.figure(figsize=(25,20))
plt.gcf().subplots_adjust( wspace = 0.2, hspace = 0.2)
#plt.rc_context({'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'})
plt.rc_context({'figure.facecolor':'white'})
i = 1
for index_i, row_i in df_points_dist_per_region.iterrows():
  if row_i.count_point > 2:
    axs = fig.add_subplot(nrows, ncols, i )
    
    axs.hist(x = df_points_dist[df_points_dist["Autonomia"] == row_i["Autonomia"]].distances.tolist(), bins=5, rwidth=0.9)
    axs.set_title('Nearest distances in : {} ({} points mean = {:.4f})'.format(row_i["Autonomia"], row_i["count_point"], row_i["mean"]))
    axs.set_xlabel('distance(km)')
    axs.set_ylabel('Points number')
    i+= 1

df_points_dist_per_region

"""It appears that in the autonomia:
* Asturia: the ~100 points are ~3 km around each other
* Valancia: the 10 points are almost at the same location 
* Adalucia: the 4 poitns
* Cantabria: the 3 points
* Pais Vasco: the 3 points
* Castilla y leon: not enough points to talk
* Murcia: not enough points to talk

2 strategies could be relevant:
* Put the new points in the biggets traffic are, like in Asturia, especially if we consider that the electric cars are used to commuting for short distances.
* Put the new points to allow to travel throughout Spain.

2 - explore the areas containing most of the points
"""

df_points.info()

list_coord_points = np.array( [[row_i["latitud"], row_i["longitud"]] for index_i, row_i in df_points.iterrows()])

# Commented out IPython magic to ensure Python compatibility.
import copy
import matplotlib

from matplotlib.colors import LogNorm
# %matplotlib inline
#cmap = copy.copy(matplotlib.cm.jet)
#cmap.set_bad((0,0,0)) 
center = [40.4165001, -3.7025599]  #  Madrid location
radius=0.1
hist_range = [
    [center[1] - radius, center[1] + radius],
    [center[0] - radius, center[0] + radius]
]
print("center : {}".format(hist_range))
plt.hist2d(list(list_coord_points[:,0]), list(list_coord_points[:,1]), 
                bins=200, 
                norm=LogNorm(),
                cmap=cmap, 
                #range = hist_range
           )
plt.grid('off')
plt.axis('off')
#fig.axes[0].get_xaxis().set_visible(False)
#fig.axes[0].get_yaxis().set_visible(False)
plt.tight_layout()
plt.show()
# c'est tout pourris

"""Plot point on a map"""

!pip install geopandas

!pip install topojson

# https://github.com/deldersveld/topojson/tree/master/countries/spain
import geopandas as gpd
import json

plt.rcParams['figure.figsize'] = (20, 10)
plt.rc_context({'figure.facecolor':'white'})
df = gpd.read_file('spain-provinces.json')
df.plot(color='grey', ax=ax,  alpha = 0.8)

#with open('spain-comunidad.json',"r") as json_file:
    #   with json.load(json_file) as json_file_load:
#df = pd.read_json('spain-comunidad.json')
crs = {'init': 'epsg:4326'}
geo_df_com = gpd.read_file('spain-comunidad.json', crs = crs)
geo_df_com.info()
#geo_df_com.scale(xfact = 0.00038734388431869277, yfact = 0.00027674749512036695)
#geo_df_com.affine_transform([0, 0, 0, 0, -9.298193999999967,35.280422000000044])
# Remove Island
geo_df_com = geo_df_com[geo_df_com["NAME_1"] != "Islas Baleares"]
geo_df_com = geo_df_com[geo_df_com["NAME_1"] != "Ceuta y Melilla"]

geo_df_com['coord_label'] = geo_df_com['geometry'].apply(lambda x: x.representative_point().coords[:])
geo_df_com['coord_label'] = [coords[0] for coords in geo_df_com['coord_label']]


#geo_df_com.head()
fig, ax = plt.subplots(1,1)
# plot background map
geo_df_com.plot(color='grey', ax = ax, alpha = 0.8)

# https://github.com/rylativity/identifying_wnv_locations-raw_repo-/blob/master/03-Geo_modeling.ipynb
from shapely.geometry import Point, Polygon
df_points_dist = pd.DataFrame()
geometry = [Point(row_i["longitud"], row_i["latitud"]) for index_i, row_i in df_merged_w_points.drop_duplicates().dropna().iterrows()
                                ]
geo_points = gpd.GeoDataFrame(geometry = geometry, crs = crs)

fig, ax = plt.subplots(1,1,figsize = (25, 25))
# plot background map
geo_df_com.plot(color='grey', ax = ax, alpha = 0.5, edgecolor='yellow', legend=True)
ax.set_title("Autonomia regions", fontsize=20)
# plot autonomia region name
for idx, row in geo_df_com.iterrows():
    plt.annotate(s=row['NAME_1'], xy=row['coord_label'],
                 horizontalalignment='center',color='blue')
# plot point
geo_points.plot(marker = '+', color='red', ax=ax, label = "Charging points", markersize = 40, legend = True)
ax.legend(title="legend")
ax.set_axis_off()
ax.set_axis_off()
# TODO : ajouter route et principales villes

# test if Madrid is in the map 
p_test = Point(40.4167754, -3.7037902)  # Madrid coords
test = [ p_test.within(poly["geometry"]) for key, poly in geo_df_com.iterrows()]
bool_test = True
for i in test : bool_test &= i
print("Madrid is inside ? : {}".format(bool_test))

"""## Dashboard

Pour le moment je fait le dashboard, je ne sais pas comment l'ajouter au projet pour que tous le monde le voit

# Premier jet
"""

#One-Hot Encoding
df_merged_dummy = pd.concat([pd.get_dummies(df_merged['Autonomia']), df_merged[['count_point', 'count_veh']]], axis=1)
df_merged_dummy

#Entrainement d'un modele KMeans
from sklearn.cluster import KMeans

n_clusters = 3
X = df_merged_dummy.values
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(X)

#Check des corrdonées des centroides
kmeans.cluster_centers_

#Dimensionality reduction
from sklearn.decomposition import PCA
# Check we can stop at Dim 2
n_components = 4
pca = PCA(n_components=n_components)
pca.fit(X)
print("\n -----explained_variance_ratio_---")
print(pca.explained_variance_ratio_)

n_components = 2
pca = PCA(n_components=n_components)
pca.fit(X)
X_reduced = pca.fit_transform(X)

#Points en 
import matplotlib.pyplot as plt

plt.scatter(X_reduced[:, 0], X_reduced[:, 1])

n_components = 2
pca_center = PCA(n_components=n_components)
center_reduced = pca_center.fit_transform(kmeans.cluster_centers_)
plt.scatter(center_reduced[:, 0], center_reduced[:, 1])

#It will be really helpful if we can present all the result we find in a dashbord
#using this link we have all the tools
#https://dash-gallery.plotly.host/dash-manufacture-spc-dashboard/
#this exemple of dahboard is really interesting !!
